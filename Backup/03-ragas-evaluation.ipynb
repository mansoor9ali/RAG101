{"cells":[{"cell_type":"markdown","metadata":{"id":"qQG4iSxVxw8f"},"source":["# RAGAS Evaluation for LangChain Agents"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1eOnr6z_zLoc"},"outputs":[],"source":["!python --version"]},{"cell_type":"markdown","metadata":{"id":"ov6TCS7bx1oI"},"source":["**R**etrieval **A**ugmented **G**eneration **As**sessment (RAGAS) is an evaluation framework for quantifying the performances of our RAG pipelines. In this example we will see how to use it with a RAG-enabled conversational agent in LangChain.\n","\n","Because we need an agent and RAG pipeline to evaluate RAGAS the first part of this notebook covers setting up an XML Agent with RAG. Jump ahead to **Integrating RAGAS** for the RAGAS section.\n","\n","To begin, let's install the prerequisites:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zshhLDrgbFKk","outputId":"26163d61-efb6-4c70-f81b-b19e1d3a943b","executionInfo":{"status":"ok","timestamp":1761993146584,"user_tz":240,"elapsed":2783,"user":{"displayName":"Mansoor Ali Syed","userId":"16693731061247144347"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n","    status = run_func(*args)\n","             ^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n","    return func(self, options, args)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/commands/install.py\", line 362, in run\n","    resolver = self.make_resolver(\n","               ^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/req_command.py\", line 177, in make_resolver\n","    return pip._internal.resolution.resolvelib.resolver.Resolver(\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 58, in __init__\n","    self.factory = Factory(\n","                   ^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 129, in __init__\n","    for dist in env.iter_installed_distributions(local_only=False)\n","                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/metadata/base.py\", line 664, in <genexpr>\n","    return (d for d in it if d.canonical_name not in skip)\n","                       ^^\n","  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/metadata/base.py\", line 612, in iter_all_distributions\n","    for dist in self._iter_distributions():\n","                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/metadata/importlib/_envs.py\", line 176, in _iter_distributions\n","    yield from finder.find(location)\n","  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/metadata/importlib/_envs.py\", line 79, in find\n","    for dist, info_location in self._find_impl(location):\n","                               ^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/metadata/importlib/_envs.py\", line 64, in _find_impl\n","    raw_name = get_dist_name(dist)\n","               ^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/metadata/importlib/_compat.py\", line 52, in get_dist_name\n","    name = cast(Any, dist).name\n","           ^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.12/importlib/metadata/__init__.py\", line 457, in name\n","    return self.metadata['Name']\n","           ^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.12/importlib/metadata/__init__.py\", line 452, in metadata\n","    return _adapters.Message(email.message_from_string(text))\n","                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.12/email/__init__.py\", line 37, in message_from_string\n","    return Parser(*args, **kws).parsestr(s)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.12/email/parser.py\", line 64, in parsestr\n","    return self.parse(StringIO(text), headersonly=headersonly)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.12/email/parser.py\", line 53, in parse\n","    feedparser.feed(data)\n","  File \"/usr/lib/python3.12/email/feedparser.py\", line 174, in feed\n","    self._call_parse()\n","  File \"/usr/lib/python3.12/email/feedparser.py\", line 178, in _call_parse\n","    self._parse()\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/pip3\", line 10, in <module>\n","    sys.exit(main())\n","             ^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n","    return command.main(cmd_args)\n","           ^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n","    return self._main(args)\n","           ^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n","    return run(options, args)\n","           ^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 215, in exc_logging_wrapper\n","    logger.critical(\"Operation cancelled by user\")\n","  File \"/usr/lib/python3.12/logging/__init__.py\", line 1586, in critical\n","    self._log(CRITICAL, msg, args, **kwargs)\n","  File \"/usr/lib/python3.12/logging/__init__.py\", line 1684, in _log\n","    self.handle(record)\n","  File \"/usr/lib/python3.12/logging/__init__.py\", line 1700, in handle\n","    self.callHandlers(record)\n","  File \"/usr/lib/python3.12/logging/__init__.py\", line 1762, in callHandlers\n","    hdlr.handle(record)\n","  File \"/usr/lib/python3.12/logging/__init__.py\", line 1028, in handle\n","    self.emit(record)\n","  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/utils/logging.py\", line 172, in emit\n","    style = Style(color=\"red\")\n","            ^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/rich/style.py\", line 122, in __init__\n","    def __init__(\n","\n","KeyboardInterrupt\n","^C\n"]}],"source":["!pip install -qU \\\n","    langchain==0.1.1 \\\n","    langchain-community==0.0.13 \\\n","    langchainhub==0.1.14 \\\n","    anthropic==0.14.0 \\\n","    cohere==4.45 \\\n","    pinecone-client==3.1.0 \\\n","    datasets==2.16.1 \\\n","    ragas==0.1.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VeosBMaIDs52"},"outputs":[],"source":["import os\n","from getpass import getpass\n","\n","# dashboard.cohere.com\n","os.environ[\"COHERE_API_KEY\"] = \"<<YOUR_KEY>>\" or getpass(\"Cohere API key: \")\n","# app.pinecone.io\n","os.environ[\"PINECONE_API_KEY\"] = \"<<YOUR_KEY>>\" or getpass(\"Pinecone API key: \")\n","# console.anthropic.com\n","os.environ[\"ANTHROPIC_API_KEY\"] = \"<<YOUR_KEY>>\" or getpass(\"Anthropic API key: \")\n","# platform.openai.com\n","os.environ[\"OPENAI_API_KEY\"] = \"<<YOUR_KEY>>\" or getpass(\"OpenAI API key: \")"]},{"cell_type":"markdown","metadata":{"id":"bpKfZkUYzQhB"},"source":["## Finding Knowledge"]},{"cell_type":"markdown","metadata":{"id":"JDTQoxcNzUa8"},"source":["The first thing we need for an agent using RAG is somewhere we want to pull knowledge from. We will use v2 of the AI ArXiv dataset, available on Hugging Face Datasets at [`jamescalam/ai-arxiv2-chunks`](https://huggingface.co/datasets/jamescalam/ai-arxiv2-chunks).\n","\n","_Note: we're using the prechunked dataset. For the raw version see [`jamescalam/ai-arxiv2`](https://huggingface.co/datasets/jamescalam/ai-arxiv2)._"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":255,"referenced_widgets":["0b1fb700ad244b779504bfc5369d149e","0dfcd16db32a41bc857e2cf8534cbdeb","9d472b6fb7cd426287339731c346125c","4feb66f8084c4fbe97eef55d6abe8912","bfd18e73f60c4e39883ddad18f24b3ca","19b50dd3d70e4622bcdccb3a31e91e02","9c5f8b85585d40738452790b141e37bc","c3a329d16ee44cc0b507f48c1222c6c0","d3645f322ebf4866937b40339dec97b2","ee1762f4b478483d9247e95f3a67f4a6","cdbfb99f4a8a4b6a98c7d93813936aea","57210955a22a4420b7376da12d7f92c9","552469f0e2554fb79fae547dc0713218","68666504b754476694c50a609bb8add1","5df8893de1534c4ab5d61a57934e9e28","69de8f56ccec49628a3ce2a1360354b1","0dff65a6ae094d87ad2fdc283498be57","7965f8c288fc40c8860ff162fd692430","97649850f46b464599bfd1d0c95dd28a","393898d6a5a1484ca31f85f8b4137ebf","6a2f1b85cea24346a0fc5e33be01b7e9","900f8f4ddf7d4829bf35767e8b98480b"]},"id":"U9gpYFnzbFKm","outputId":"17d56bbc-7131-4af4-8aaf-003b1ac4870e","executionInfo":{"status":"ok","timestamp":1761993067266,"user_tz":240,"elapsed":20351,"user":{"displayName":"Mansoor Ali Syed","userId":"16693731061247144347"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["train.jsonl:   0%|          | 0.00/766M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b1fb700ad244b779504bfc5369d149e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/240927 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57210955a22a4420b7376da12d7f92c9"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n","    num_rows: 20000\n","})"]},"metadata":{},"execution_count":1}],"source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"jamescalam/ai-arxiv2-chunks\", split=\"train[:20000]\")\n","dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_bP7ZW-ybFKm","executionInfo":{"status":"ok","timestamp":1761993071053,"user_tz":240,"elapsed":17,"user":{"displayName":"Mansoor Ali Syed","userId":"16693731061247144347"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"73cca7b4-0ed6-4139-903b-f3cad52bb367"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'doi': '2401.09350',\n"," 'chunk-id': 1,\n"," 'chunk': 'These neural networks and their training algorithms may be complex, and the scope of their impact broad and wide, but nonetheless they are simply functions in a high-dimensional space. A trained neural network takes a vector as input, crunches and transforms it in various ways, and produces another vector, often in some other space. An image may thereby be turned into a vector, a song into a sequence of vectors, and a social network as a structured collection of vectors. It seems as though much of human knowledge, or at least what is expressed as text, audio, image, and video, has a vector representation in one form or another.\\nIt should be noted that representing data as vectors is not unique to neural networks and deep learning. In fact, long before learnt vector representations of pieces of dataâ\\x80\\x94what is commonly known as â\\x80\\x9cembeddingsâ\\x80\\x9dâ\\x80\\x94came along, data was often encoded as hand-crafted feature vectors. Each feature quanti- fied into continuous or discrete values some facet of the data that was deemed relevant to a particular task (such as classification or regression). Vectors of that form, too, reflect our understanding of a real-world object or concept.',\n"," 'id': '2401.09350#1',\n"," 'title': 'Foundations of Vector Retrieval',\n"," 'summary': 'Vectors are universal mathematical objects that can represent text, images,\\nspeech, or a mix of these data modalities. That happens regardless of whether\\ndata is represented by hand-crafted features or learnt embeddings. Collect a\\nlarge enough quantity of such vectors and the question of retrieval becomes\\nurgently relevant: Finding vectors that are more similar to a query vector.\\nThis monograph is concerned with the question above and covers fundamental\\nconcepts along with advanced data structures and algorithms for vector\\nretrieval. In doing so, it recaps this fascinating topic and lowers barriers of\\nentry into this rich area of research.',\n"," 'source': 'http://arxiv.org/pdf/2401.09350',\n"," 'authors': 'Sebastian Bruch',\n"," 'categories': 'cs.DS, cs.IR',\n"," 'comment': None,\n"," 'journal_ref': None,\n"," 'primary_category': 'cs.DS',\n"," 'published': '20240117',\n"," 'updated': '20240117',\n"," 'references': []}"]},"metadata":{},"execution_count":2}],"source":["dataset[1]"]},{"cell_type":"markdown","metadata":{"id":"VX6NdQhgbFKn"},"source":["## Building the Knowledge Base"]},{"cell_type":"markdown","metadata":{"id":"MDCbqQl_bFKn"},"source":["To build our knowledge base we need _two things_:\n","\n","1. Embeddings, for this we will use `CohereEmbeddings` using Cohere's embedding models, which do need an [API key](https://dashboard.cohere.com/api-keys).\n","2. A vector database, where we store our embeddings and query them. We use Pinecone which again requires a [free API key](https://app.pinecone.io).\n","\n","First we initialize our connection to Cohere and define an `embed` helper function:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wkw0KyLRbFKo"},"outputs":[],"source":["from langchain_community.embeddings import CohereEmbeddings\n","\n","embed = CohereEmbeddings(model=\"embed-english-v3.0\")"]},{"cell_type":"markdown","metadata":{"id":"LhDzfsczbFKo"},"source":["Then we initialize our connection to Pinecone:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j0N7EcJibFKo"},"outputs":[],"source":["from pinecone import Pinecone\n","\n","# configure client\n","pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])"]},{"cell_type":"markdown","metadata":{"id":"g65RLGIpbFKo"},"source":["Now we setup our index specification, this allows us to define the cloud provider and region where we want to deploy our index. You can find a list of all [available providers and regions here](https://docs.pinecone.io/docs/projects)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8stIZYKdbFKo"},"outputs":[],"source":["from pinecone import ServerlessSpec\n","\n","spec = ServerlessSpec(\n","    cloud=\"aws\", region=\"us-west-2\"\n",")"]},{"cell_type":"markdown","metadata":{"id":"-8Ep3743bFKo"},"source":["Before creating an index, we need the dimensionality of our Cohere embedding model, which we can find easily by creating an embedding and checking the length:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DwMhLWLDbFKo"},"outputs":[],"source":["vec = embed.embed_documents([\"ello\"])\n","len(vec[0])"]},{"cell_type":"markdown","metadata":{"id":"G3X7nZIabFKp"},"source":["Now we create the index using our embedding dimensionality, and a metric also compatible with the model (this can be either cosine or dotproduct). We also pass our spec to index initialization."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E6Bl7xTJbFKp"},"outputs":[],"source":["import time\n","\n","index_name = \"ragas-evaluation\"\n","\n","# check if index already exists (it shouldn't if this is first time)\n","if index_name not in pc.list_indexes().names():\n","    # if does not exist, create index\n","    pc.create_index(\n","        index_name,\n","        dimension=len(vec[0]),  # dimensionality of cohere v3\n","        metric='dotproduct',\n","        spec=spec\n","    )\n","    # wait for index to be initialized\n","    while not pc.describe_index(index_name).status['ready']:\n","        time.sleep(1)\n","\n","# connect to index\n","index = pc.Index(index_name)\n","time.sleep(1)\n","# view index stats\n","index.describe_index_stats()"]},{"cell_type":"markdown","metadata":{"id":"6ZUn2lu7bFKp"},"source":["### Populating our Index"]},{"cell_type":"markdown","metadata":{"id":"PeVD6d0sbFKp"},"source":["Now our knowledge base is ready to be populated with our data. We will use the `embed` helper function to embed our documents and then add them to our index.\n","\n","We will also include metadata from each record."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hb00VSTqbFKp"},"outputs":[],"source":["from tqdm.auto import tqdm\n","\n","# easier to work with dataset as pandas dataframe\n","data = dataset.to_pandas()\n","\n","batch_size = 100\n","\n","for i in tqdm(range(0, len(data), batch_size)):\n","    i_end = min(len(data), i+batch_size)\n","    # get batch of data\n","    batch = data.iloc[i:i_end]\n","    # generate unique ids for each chunk\n","    ids = [x[\"id\"] for i, x in batch.iterrows()]\n","    # get text to embed\n","    texts = [x['chunk'] for _, x in batch.iterrows()]\n","    # embed text\n","    embeds = embed.embed_documents(texts)\n","    # get metadata to store in Pinecone\n","    metadata = [\n","        {'text': x['chunk'],\n","         'source': x['source'],\n","         'title': x['title']} for i, x in batch.iterrows()\n","    ]\n","    # add to Pinecone\n","    index.upsert(vectors=zip(ids, embeds, metadata))"]},{"cell_type":"markdown","metadata":{"id":"z6VVT3X_EMDO"},"source":["Create a tool for our agent to use when searching for ArXiv papers:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X9J5jHKcEQz6"},"outputs":[],"source":["from langchain.agents import tool\n","\n","@tool\n","def arxiv_search(query: str) -> str:\n","    \"\"\"Use this tool when answering questions about AI, machine learning, data\n","    science, or other technical questions that may be answered using arXiv\n","    papers.\n","    \"\"\"\n","    # create query vector\n","    xq = embed.embed_query(query)\n","    # perform search\n","    out = index.query(vector=xq, top_k=5, include_metadata=True)\n","    # reformat results into string\n","    results_str = \"\\n---\\n\".join(\n","        [x[\"metadata\"][\"text\"] for x in out[\"matches\"]]\n","    )\n","    return results_str\n","\n","tools = [arxiv_search]"]},{"cell_type":"markdown","metadata":{"id":"uN7d_4r-JMPW"},"source":["When this tool is used by our agent it will execute it like so:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eq4H-2RpI1U3"},"outputs":[],"source":["print(\n","    arxiv_search.run(tool_input={\"query\": \"can you tell me about llama 2?\"})\n",")"]},{"cell_type":"markdown","metadata":{"id":"XUvJOqrNhYIh"},"source":["## Defining XML Agent"]},{"cell_type":"markdown","metadata":{"id":"s45dwd78hbvk"},"source":["The XML agent is built primarily to support Anthropic models. Anthropic models have been trained to use XML tags like `<input>{some input}</input` or when using a tool they use:\n","\n","```\n","<tool>{tool name}</tool>\n","<tool_input>{tool input}</tool_input>\n","```\n","\n","This is much different to the format produced by typical ReAct agents, which is not as well supported by Anthropic models.\n","\n","To create an XML agent we need a `prompt`, `llm`, and list of `tools`. We can download a prebuilt prompt for conversational XML agents from LangChain hub."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ntuT7UuXeMz0"},"outputs":[],"source":["from langchain import hub\n","\n","prompt = hub.pull(\"hwchase17/xml-agent-convo\")\n","prompt"]},{"cell_type":"markdown","metadata":{"id":"rfdcKCdwi0SL"},"source":["We can see the XML format being used throughout the prompt when explaining to the LLM how it should use tools."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kDHuU2uOdW91"},"outputs":[],"source":["from langchain_community.chat_models import ChatAnthropic\n","\n","# chat completion llm\n","llm = ChatAnthropic(\n","    anthropic_api_key=os.environ[\"ANTHROPIC_API_KEY\"],\n","    model_name='claude-2.1',\n","    temperature=0.0\n",")"]},{"cell_type":"markdown","metadata":{"id":"g33Nt-xijPKG"},"source":["When the agent is run we will provide it with a single `input` — this is the input text from a user. However, within the agent logic an *agent_scratchpad* object will be passed too, which will include tool information. To feed this information into our LLM we will need to transform it into the XML format described above, we define the `convert_intermediate_steps` function to handle that."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TMMBgMBlIJoq"},"outputs":[],"source":["def convert_intermediate_steps(intermediate_steps):\n","    log = \"\"\n","    for action, observation in intermediate_steps:\n","        log += (\n","            f\"<tool>{action.tool}</tool><tool_input>{action.tool_input}\"\n","            f\"</tool_input><observation>{observation}</observation>\"\n","        )\n","    return log"]},{"cell_type":"markdown","metadata":{"id":"T5_PQWVckAOi"},"source":["We must also parse the tools into a string containing `tool_name: tool_description` — we handle that with the `convert_tools` function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qxbrF5a4j9il"},"outputs":[],"source":["def convert_tools(tools):\n","    return \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in tools])"]},{"cell_type":"markdown","metadata":{"id":"SCVI2dyUIRg6"},"source":["With everything ready we can go ahead and initialize our agent object using [**L**ang**C**hain **E**xpression **L**anguage (LCEL)](https://www.pinecone.io/learn/series/langchain/langchain-expression-language/). We add instructions for when the LLM should _stop_ generating with `llm.bind(stop=[...])` and finally we parse the output from the agent using an `XMLAgentOutputParser` object."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z3yhTDmEIU4n"},"outputs":[],"source":["from langchain.agents.output_parsers import XMLAgentOutputParser\n","\n","agent = (\n","    {\n","        \"input\": lambda x: x[\"input\"],\n","        # without \"chat_history\", tool usage has no context of prev interactions\n","        \"chat_history\": lambda x: x[\"chat_history\"],\n","        \"agent_scratchpad\": lambda x: convert_intermediate_steps(\n","            x[\"intermediate_steps\"]\n","        ),\n","    }\n","    | prompt.partial(tools=convert_tools(tools))\n","    | llm.bind(stop=[\"</tool_input>\", \"</final_answer>\"])\n","    | XMLAgentOutputParser()\n",")"]},{"cell_type":"markdown","metadata":{"id":"MG2_hL4hkudq"},"source":["With our `agent` object initialized we pass it to an `AgentExecutor` object alongside our original `tools` list:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YHW_K3WOIsXw"},"outputs":[],"source":["from langchain.agents import AgentExecutor\n","\n","agent_executor = AgentExecutor(\n","    agent=agent, tools=tools, return_intermediate_steps=True\n",")"]},{"cell_type":"markdown","metadata":{"id":"QRCtHauRlkLc"},"source":["Now we can use the agent via the `invoke` method:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y_Aqp20qloj7"},"outputs":[],"source":["agent_executor.invoke({\n","    \"input\": \"can you tell me about llama 2?\",\n","    \"chat_history\": \"\"\n","})"]},{"cell_type":"markdown","metadata":{"id":"BvpyjfUwnBLx"},"source":["We have no `\"chat_history\"` so we will pass an empty string to our `invoke` method:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KpKMRBMimEOt"},"outputs":[],"source":["user_msg = \"hello mate\"\n","\n","out = agent_executor.invoke({\n","    \"input\": \"hello mate\",\n","    \"chat_history\": \"\"\n","})"]},{"cell_type":"markdown","metadata":{"id":"q0L_80WrpWqd"},"source":["Now let's put together another helper function called `chat` to help us handle the _state_ part of our agent."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C-Ck2Lv53rD-"},"outputs":[],"source":["def chat(text: str):\n","    out = agent_executor.invoke({\n","        \"input\": text,\n","        \"chat_history\": \"\"\n","    })\n","    return out"]},{"cell_type":"markdown","metadata":{"id":"XIheLeTBsO9S"},"source":["Now we simply chat with our agent and it will remember the context of previous interactions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iJ_PH7YcA_f2"},"outputs":[],"source":["print(chat(\"can you tell me about llama 2?\")[\"output\"])"]},{"cell_type":"markdown","metadata":{"id":"5p8m4Gc5w1OX"},"source":["We can ask follow up questions that miss key information but thanks to the conversational history the LLM understands the context and uses that to adjust the search query.\n","\n","_Note: if missing `\"chat_history\"` parameter from the `agent` definition you will likely notice a lack of context in the search term, and in some cases this lack of good information can trigger a `ValueError` during output parsing._"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3XJ_3JIgBDRl"},"outputs":[],"source":["out = chat(\"was any red teaming done?\")\n","print(out[\"output\"])"]},{"cell_type":"markdown","metadata":{"id":"SelG8OcOxggP"},"source":["We get a reasonable answer here. It's worth noting that with previous iterations of this test, ie \"llama 2 red teaming\" using the original `ai-arxiv` dataset rarely (if ever) returned directly relevant results."]},{"cell_type":"markdown","metadata":{"id":"e9bI9czPtWnl"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"gNHpkMPoDKEV"},"source":["## Integrating RAGAS"]},{"cell_type":"markdown","metadata":{"id":"IuyybSe5FlZg"},"source":["To integrate RAGAS evaluation into this pipeline we need a few things, from our pipeline we need the retrieved contexts, and the generated output.\n","\n","We already have the generated output, it is what we're printing above. However, the retrieved contexts are being logged but we haven't seen how to programatically extract them yet. Let's take a look at what we are returned in `out`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DPJkvJh1DJSD"},"outputs":[],"source":["out"]},{"cell_type":"markdown","metadata":{"id":"qjFyIBvFLcIo"},"source":["When initializing our `AgentExecutor` object we included `return_intermediate_steps=True` — this (unsuprisingly) returns the intermediate steps that the agent tool to generate the final answer. Those steps include the response from our `arxiv_search` tool — which we can use the evaluate the retrieval portion of our pipeline with RAGAS."]},{"cell_type":"markdown","metadata":{"id":"UwCU9B1rOkZ9"},"source":["We extract the contexts themselves like so:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"He3I1jAGOnI9"},"outputs":[],"source":["print(out[\"intermediate_steps\"][0][1])"]},{"cell_type":"markdown","metadata":{"id":"wuhMvGdBOnvu"},"source":["## Evaluation"]},{"cell_type":"markdown","metadata":{"id":"bczg8jREMwLw"},"source":["To evaluate with RAG we need a dataset containing question, ideal contexts, and the _ground truth_ answers to those questions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ee-LezlRGSsu"},"outputs":[],"source":["ragas_data = load_dataset(\"aurelio-ai/ai-arxiv2-ragas-mixtral\", split=\"train\")\n","ragas_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DsSjPe0MNRi1"},"outputs":[],"source":["ragas_data[0]"]},{"cell_type":"markdown","metadata":{"id":"sU4QrqiPOF-w"},"source":["We first iterate through the questions in this evaluation dataset and ask these questions to our agent."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rfK4Y2kGNUUD"},"outputs":[],"source":["import pandas as pd\n","from tqdm.auto import tqdm\n","\n","df = pd.DataFrame({\n","    \"question\": [],\n","    \"contexts\": [],\n","    \"answer\": [],\n","    \"ground_truth\": []\n","})\n","\n","limit = 5\n","\n","for i, row in tqdm(enumerate(ragas_data), total=limit):\n","    if i >= limit:\n","        break\n","    question = row[\"question\"]\n","    ground_truths = row[\"ground_truth\"]\n","    try:\n","        out = chat(question)\n","        answer = out[\"output\"]\n","        if len(out[\"intermediate_steps\"]) != 0:\n","            contexts = out[\"intermediate_steps\"][0][1].split(\"\\n---\\n\")\n","        else:\n","            # this is where no intermediate steps are used\n","            contexts = []\n","    except ValueError:\n","        answer = \"ERROR\"\n","        contexts = []\n","    df = pd.concat([df, pd.DataFrame({\n","        \"question\": question,\n","        \"answer\": answer,\n","        \"contexts\": [contexts],\n","        \"ground_truth\": ground_truths\n","    })], ignore_index=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QrpnyBfuj0uL"},"outputs":[],"source":["df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sGBS3rApYNIt"},"outputs":[],"source":["from datasets import Dataset\n","from ragas.metrics import (\n","    faithfulness,\n","    answer_relevancy,\n","    context_precision,\n","    context_relevancy,\n","    context_recall,\n","    answer_similarity,\n","    answer_correctness,\n",")\n","\n","eval_data = Dataset.from_dict(df)\n","eval_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gckieCTmzfg1"},"outputs":[],"source":["from ragas import evaluate\n","\n","result = evaluate(\n","    dataset=eval_data,\n","    metrics=[\n","        faithfulness,\n","        answer_relevancy,\n","        context_precision,\n","        context_relevancy,\n","        context_recall,\n","        answer_similarity,\n","        answer_correctness,\n","    ],\n",")\n","result = result.to_pandas()"]},{"cell_type":"markdown","metadata":{"id":"eKuXPfTYwTr3"},"source":["### Retrieval Metrics"]},{"cell_type":"markdown","metadata":{"id":"t-pJwPjqwVrU"},"source":["Retrieval is the first step in a RAG pipeline, so we will focus on metrics that assess retrieval first. For that we primarily want to focus on `context_recall` and `context_precision` but before diving into these metrics we must understand what it is that they will be measuring.\n","\n","### Actual vs. Predicted\n","\n","When evaluating the performance of retrieval systems we tend to compare the _actual_ (ground truth) to _predicted_ results. We define these as:\n","\n","* **Actual condition** is the true label of every context in the dataset. These are _positive_ ($p$) if the context is relevant to our query or _negative_ ($n$) if the context is _ir_relevant to our query.\n","\n","* **Predicted condition** is the _predicted_ label determined by our retrieval system. If a context is returned it is a predicted _positive_, ie $\\hat{p}$. If a context is not returned it is a predicted _negative_, ie $\\hat{n}$.\n","\n","Given these conditions, we can say the following:\n","\n","* $p\\hat{p}$ is a **true positive**, meaning a relevant result has been returned.\n","* $n\\hat{n}$ is a **true negative**, meaning an irrelevant result was not returned\n","* $n\\hat{p}$ is a **false positive**, meaning an irrelevant result has been returned.\n","* $p\\hat{n}$ is a **false negative**, meaning an relevant result has _not_ been returned.\n","\n","Let's see how these apply to our metrics in RAGAS."]},{"cell_type":"markdown","metadata":{"id":"6MMDiXNOwi2n"},"source":["#### Context Recall"]},{"cell_type":"markdown","metadata":{"id":"-fHPVNBkwlMA"},"source":["Context recall (or just _recall_) is a measure of how many of the relevant records in a dataset have been retrieved. It is calculated as:\n","\n","$$\n","Recall@K = \\frac{p\\hat{p}}{p\\hat{p} + n\\hat{n}} = \\frac{Relevant \\: contexts \\: retrieved}{Total \\: number \\: of \\: relevant \\: contexts}\n","$$\n","\n","RAGAS calculates _Recall@K_ for recall, where the _@K_ represents the number of contexts returned. As the @K value is increased the recall scores will improve (as the capture size of the retrieval step increases). At it's extreme we could set @K equal to the size of the dataset to guarantee perfect recall — although this negates the point of RAG in the first place.\n","\n","By default, RAGAS uses a _@K_ value of `5`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZvbZv1IL3uR4"},"outputs":[],"source":["pd.set_option(\"display.max_colwidth\", 700)\n","result[[\"question\", \"contexts\", \"answer\", \"context_recall\"]]"]},{"cell_type":"markdown","metadata":{"id":"MA3mHXDO4ALO"},"source":["Here we can see all but the second set of results returned all relevant contexts. The score here is `0.6` meaning that 3/5 (60%) of the relevant contexts were returned.\n","\n","All other results returned `1.0` (100%), meaning all contexts were retrieved.\n","\n","Recall is a useful metric but easily fooled by simply returning more records, ie increasing the _@K_ value. Because of that it is typically paired with _precision_."]},{"cell_type":"markdown","metadata":{"id":"xmItVH9l_BeD"},"source":["### Context Precision"]},{"cell_type":"markdown","metadata":{"id":"Fnr0tS-7_Dho"},"source":["Context precision (or just _precision_) is another popular retrieval metric. We typically see both recall and precision paired together when evaluating retrieval systems.\n","\n","As with recall, the actual metric here is called _Precision@K_ where @K represents the number of contexts returned. However, unlike recall, precision is focusing on the number of relevant results returned compared to the total results returned, whether they are relevant or not — this is equal to our chosen _@K_ value.\n","\n","$$\n","Precision@K = \\frac{p\\hat{p}}{p\\hat{p} + p\\hat{n}} = \\frac{Relevant \\: contexts \\: retrieved}{Total \\: number \\: of \\: relevant \\: contexts}\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M6ZZgWPKWpw6"},"outputs":[],"source":["pd.set_option(\"display.max_colwidth\", 700)\n","result[[\"question\", \"contexts\", \"answer\", \"context_precision\"]]"]},{"cell_type":"markdown","metadata":{"id":"vD6NoHDZWyVx"},"source":["Our precision@K scores are equal to our recall scores (this can happen when there are _5_ relevant contexts for each query at we set _@K = 5_). This result means every query produced 100% precision with the exception of our 60% precision result where only 3/5 returned contexts were relevant."]},{"cell_type":"markdown","metadata":{"id":"NI5NgDpWXKuY"},"source":["## Generation Metrics"]},{"cell_type":"markdown","metadata":{"id":"kTUWxra6wK_A"},"source":["### Faithfullness"]},{"cell_type":"markdown","metadata":{"id":"pBqu0ZBGgAwl"},"source":["The _faithfullness_ metric measures (from _0_ to _1_) the factual consistency of an answer when compared to the retrieved context. A score of _1_ means all claims in the answer can be found in the context. A score of _0_ would indicate _no_ claims in the answer are found in the context.\n","\n","We calculate the faithfullness like so:\n","\n","$$\n","Faithfulness = \\frac{Number \\: of \\: claims \\: in \\: answer \\: also \\: found \\: in \\: context}{Number \\: of \\: claims \\: in \\: answer}\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0rGhG62Hv2bm"},"outputs":[],"source":["pd.set_option(\"display.max_colwidth\", 1000)\n","result[[\"question\", \"contexts\", \"answer\", \"faithfulness\"]]"]},{"cell_type":"markdown","metadata":{"id":"xQrPEENFg4j7"},"source":["When calculating faithfullness RAGAS is using OpenAI LLMs to decide which claims are in the answer and whether they also exist in the context. Because of the \"generative\" nature of this approach we won't always get accurate scores.\n","\n","We can see that we get perfect scores for all but our fourth result, which scores `0.0`. However, when looking at this we can see some claims that seem related. Nonetheless the fourth answer does seem to be less grounded in the truth of our context than other responses, indicated that there is justification behind this low score."]},{"cell_type":"markdown","metadata":{"id":"JxZEFhW0h1FE"},"source":["### Answer Relevancy"]},{"cell_type":"markdown","metadata":{"id":"IIA7gdXqh2wF"},"source":["Answer relevancy is our final metric. It focuses on the generation component and is similar to our \"context precision\" metric in that it measures how much of the returned information is relevant to our original question.\n","\n","We return a low answer relevancy score when:\n","\n","* Answers are incomplete.\n","\n","* Answers contain redundant information.\n","\n","A high answer relevancy score indicates that an answer is concise and does not contain \"fluff\" (ie irrelevant information).\n","\n","The score is calculated by asking an LLM to generate multiple questions for a generated answer and then calculating the cosine similarity between the original question and the generated questions. Naturally, if we have a concise answer that answers a very specific question, we should find that the generated question will have a high cosine similarity to the original question."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G_w-A1WOzddF"},"outputs":[],"source":["pd.set_option(\"display.max_colwidth\", 700)\n","result[[\"question\", \"answer\", \"answer_relevancy\"]]"]},{"cell_type":"markdown","metadata":{"id":"NCXWYnBlr69a"},"source":["Again we can see poorer performance from our fourth answer but the remainder (particularly answer with similarity greater than `0.9`) perform well."]},{"cell_type":"markdown","metadata":{"id":"CVaNDvKwl7BE"},"source":["---"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"0b1fb700ad244b779504bfc5369d149e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0dfcd16db32a41bc857e2cf8534cbdeb","IPY_MODEL_9d472b6fb7cd426287339731c346125c","IPY_MODEL_4feb66f8084c4fbe97eef55d6abe8912"],"layout":"IPY_MODEL_bfd18e73f60c4e39883ddad18f24b3ca"}},"0dfcd16db32a41bc857e2cf8534cbdeb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19b50dd3d70e4622bcdccb3a31e91e02","placeholder":"​","style":"IPY_MODEL_9c5f8b85585d40738452790b141e37bc","value":"train.jsonl: 100%"}},"9d472b6fb7cd426287339731c346125c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3a329d16ee44cc0b507f48c1222c6c0","max":765731119,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d3645f322ebf4866937b40339dec97b2","value":765731119}},"4feb66f8084c4fbe97eef55d6abe8912":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee1762f4b478483d9247e95f3a67f4a6","placeholder":"​","style":"IPY_MODEL_cdbfb99f4a8a4b6a98c7d93813936aea","value":" 766M/766M [00:07&lt;00:00, 250MB/s]"}},"bfd18e73f60c4e39883ddad18f24b3ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19b50dd3d70e4622bcdccb3a31e91e02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c5f8b85585d40738452790b141e37bc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c3a329d16ee44cc0b507f48c1222c6c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3645f322ebf4866937b40339dec97b2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ee1762f4b478483d9247e95f3a67f4a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cdbfb99f4a8a4b6a98c7d93813936aea":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"57210955a22a4420b7376da12d7f92c9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_552469f0e2554fb79fae547dc0713218","IPY_MODEL_68666504b754476694c50a609bb8add1","IPY_MODEL_5df8893de1534c4ab5d61a57934e9e28"],"layout":"IPY_MODEL_69de8f56ccec49628a3ce2a1360354b1"}},"552469f0e2554fb79fae547dc0713218":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0dff65a6ae094d87ad2fdc283498be57","placeholder":"​","style":"IPY_MODEL_7965f8c288fc40c8860ff162fd692430","value":"Generating train split: 100%"}},"68666504b754476694c50a609bb8add1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_97649850f46b464599bfd1d0c95dd28a","max":240927,"min":0,"orientation":"horizontal","style":"IPY_MODEL_393898d6a5a1484ca31f85f8b4137ebf","value":240927}},"5df8893de1534c4ab5d61a57934e9e28":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6a2f1b85cea24346a0fc5e33be01b7e9","placeholder":"​","style":"IPY_MODEL_900f8f4ddf7d4829bf35767e8b98480b","value":" 240927/240927 [00:05&lt;00:00, 53545.25 examples/s]"}},"69de8f56ccec49628a3ce2a1360354b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0dff65a6ae094d87ad2fdc283498be57":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7965f8c288fc40c8860ff162fd692430":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"97649850f46b464599bfd1d0c95dd28a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"393898d6a5a1484ca31f85f8b4137ebf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6a2f1b85cea24346a0fc5e33be01b7e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"900f8f4ddf7d4829bf35767e8b98480b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}